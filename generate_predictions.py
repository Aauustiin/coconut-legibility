import argparse
import json
import re
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def parse_args():
    parser = argparse.ArgumentParser(
        description="Load a HuggingFace model and generate predictions using vLLM"
    )
    parser.add_argument(
        "--model-id",
        type=str,
        required=True,
        help="HuggingFace model identifier (e.g., 'Qwen/Qwen3-1.7B')"
    )
    parser.add_argument(
        "--representations-file",
        type=str,
        required=True,
        help="Path to the text representations JSON file generated by generate_representations.py"
    )
    parser.add_argument(
        "--legibility-results",
        type=str,
        default="legibility_results.json",
        help="Path to the legibility results JSON file (default: legibility_results.json)"
    )
    parser.add_argument(
        "--prompt-template",
        type=str,
        default="monitor_prompt.txt",
        help="Path to the prompt template file (default: monitor_prompt.txt)"
    )
    parser.add_argument(
        "--output-file",
        type=str,
        default="monitor_predictions.json",
        help="Path to save predictions (default: monitor_predictions.json)"
    )
    parser.add_argument(
        "--tensor-parallel-size",
        type=int,
        default=1,
        help="Number of GPUs for tensor parallelism (default: 1)"
    )
    parser.add_argument(
        "--disable-thinking",
        action="store_true",
        help="Disable Qwen3 thinking mode for faster inference"
    )
    return parser.parse_args()


def extract_prediction(response: str) -> str:
    """Extract YES/NO prediction from response."""
    pattern = r"<answer>\s*(YES|NO)\s*</answer>"
    matches = re.findall(pattern, response, re.IGNORECASE)
    if matches:
        return matches[-1].upper()
    return "UNKNOWN"


def main():
    args = parse_args()

    # Load tokenizer (needed for chat template)
    print(f"Loading tokenizer: {args.model_id}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_id)

    # Load vLLM model
    print(f"Loading vLLM model: {args.model_id}")
    llm = LLM(
        model=args.model_id,
        tensor_parallel_size=args.tensor_parallel_size,
        trust_remote_code=True,
        seed=42,
        # Optimize for throughput with a small model
        max_model_len=8192,  # Adjust if your prompts are longer
        gpu_memory_utilization=0.9,
    )

    # Load data files
    print(f"Loading text representations from: {args.representations_file}")
    with open(args.representations_file, "r") as f:
        text_representations = json.load(f)

    print(f"Loading legibility results from: {args.legibility_results}")
    with open(args.legibility_results, "r") as f:
        legibility_results = json.load(f)

    print(f"Loading prompt template from: {args.prompt_template}")
    with open(args.prompt_template, "r") as f:
        prompt_template = f.read()

    # Prepare all prompts upfront
    print(f"\n{'='*80}")
    print(f"Preparing {len(legibility_results)} prompts...")
    print(f"{'='*80}\n")

    all_prompts = []
    for i, result in enumerate(legibility_results):
        text_rep = text_representations[i]

        # Fill in the prompt template
        prompt_content = prompt_template.format(
            question=result["question"],
            ground_truth=result["ground_truth_answer"],
            reasoning=result["model_reasoning"],
            representation=text_rep
        )

        # Optionally disable thinking mode for Qwen3
        if args.disable_thinking:
            prompt_content += "\n/no_think"

        # Apply chat template
        messages = [{"role": "user", "content": prompt_content}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        all_prompts.append(formatted_prompt)

    # Set up sampling parameters
    sampling_params = SamplingParams(
        max_tokens=4096,
        temperature=0.6,  # Qwen3 default
        top_p=0.95,       # Qwen3 default
        seed=42,
    )

    # Generate all responses in one batched call
    print(f"Generating predictions for {len(all_prompts)} questions...")
    print("(vLLM will automatically batch and optimize)")
    print()

    outputs = llm.generate(all_prompts, sampling_params)

    # Process results
    print(f"\n{'='*80}")
    print("Processing results...")
    print(f"{'='*80}\n")

    predictions = []
    correct_predictions = 0
    total_known = 0

    for i, output in enumerate(outputs):
        response = output.outputs[0].text
        prediction = extract_prediction(response)
        actual_correct = legibility_results[i]["is_correct"]

        predictions.append({
            "question_idx": i,
            "prediction": prediction,
            "full_response": response.strip(),
            "actual_correct": actual_correct
        })

        # Track accuracy
        if prediction != "UNKNOWN":
            total_known += 1
            predicted_correct = (prediction == "YES")
            if predicted_correct == actual_correct:
                correct_predictions += 1

        # Progress update every 100 questions
        if (i + 1) % 100 == 0 or i == len(outputs) - 1:
            print(f"Processed {i + 1}/{len(outputs)} questions")

    # Save predictions
    with open(args.output_file, "w") as f:
        json.dump(predictions, f, indent=2)

    # Print summary
    print(f"\n{'='*80}")
    print("SUMMARY")
    print(f"{'='*80}")
    print(f"Total questions: {len(predictions)}")
    print(f"Predictions made: {total_known} ({len(predictions) - total_known} UNKNOWN)")
    if total_known > 0:
        accuracy = correct_predictions / total_known * 100
        print(f"Accuracy (on known predictions): {correct_predictions}/{total_known} ({accuracy:.1f}%)")
    print(f"\nPredictions saved to: {args.output_file}")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()