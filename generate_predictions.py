import argparse
import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


def parse_args():
    parser = argparse.ArgumentParser(
        description="Load a HuggingFace model and representations file"
    )
    parser.add_argument(
        "--model-id",
        type=str,
        required=True,
        help="HuggingFace model identifier (e.g., 'openai-community/gpt2')"
    )
    parser.add_argument(
        "--representations-file",
        type=str,
        required=True,
        help="Path to the text representations JSON file generated by generate_representations.py"
    )
    parser.add_argument(
        "--legibility-results",
        type=str,
        default="legibility_results.json",
        help="Path to the legibility results JSON file (default: legibility_results.json)"
    )
    parser.add_argument(
        "--prompt-template",
        type=str,
        default="monitor_prompt.txt",
        help="Path to the prompt template file (default: monitor_prompt.txt)"
    )
    return parser.parse_args()


def main():
    args = parse_args()

    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)

    print(f"Loading model: {args.model_id}")
    # load the tokenizer and the model
    tokenizer = AutoTokenizer.from_pretrained(args.model_id)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_id,
        torch_dtype="auto",
        device_map="auto"
    )

    print(f"Loading text representations from: {args.representations_file}")
    with open(args.representations_file, "r") as f:
        text_representations = json.load(f)

    print(f"Loading legibility results from: {args.legibility_results}")
    with open(args.legibility_results, "r") as f:
        legibility_results = json.load(f)

    print(f"Loading prompt template from: {args.prompt_template}")
    with open(args.prompt_template, "r") as f:
        prompt_template = f.read()

    print(f"\n{'='*80}")
    print("Generating predictions for each question...")
    print(f"{'='*80}\n")

    predictions = []

    for i, result in enumerate(legibility_results):
        print(f"\nProcessing question {i+1}/{len(legibility_results)}")
        print(f"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
        print(f"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB")

        # Get the corresponding text representation (already formatted)
        text_rep = text_representations[i]

        # Fill in the prompt template
        prompt = prompt_template.format(
            question=result["question"],
            ground_truth=result["ground_truth_answer"],
            reasoning=result["model_reasoning"],
            representation=text_rep
        )

        messages = [
            {"role": "user", "content": prompt}
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        # conduct text completion
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=4096
        )

        # Decode the response
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 
        response = tokenizer.decode(output_ids, skip_special_tokens=False)

        # Extract YES/NO prediction
        pattern = r"<answer>\s*(YES|NO)\s*</answer>"
        matches = re.findall(pattern, response, re.IGNORECASE)
        if matches:
            prediction = matches[-1].upper()
        else:
            prediction = "UNKNOWN"

        predictions.append({
            "question_idx": i,
            "prediction": prediction,
            "full_response": response.strip(),
            "actual_correct": result["is_correct"]
        })

        print(f"  Actual: {'CORRECT' if result['is_correct'] else 'INCORRECT'}")
        print(f"  Predicted: {prediction}")
        print(f"  Response: {response.strip()}")

    # Save predictions
    with open("monitor_predictions.json", "w") as f:
        json.dump(predictions, f, indent=2)

    print(f"\n{'='*80}")
    print("Predictions saved to monitor_predictions.json")
    print(f"{'='*80}")

    return model, tokenizer, text_representations, legibility_results, predictions


if __name__ == "__main__":
    model, tokenizer, text_representations, legibility_results, predictions = main()
